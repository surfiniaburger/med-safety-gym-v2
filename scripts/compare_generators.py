"""
Compare Data Generators: GPT-OSS vs Qwen-Coder
Uses a Judge LLM to evaluate the quality of synthetic data generated by different models.
"""

import json
import argparse
import asyncio
import os
from typing import List, Dict
import litellm
from tqdm.asyncio import tqdm

# Judge Model
JUDGE_MODEL = "ollama/qwen3-coder:480b-cloud"

JUDGE_PROMPT = """
You are an expert evaluator for medical training data.
Your task is to evaluate a generated training example for quality, schema compliance, and grounding.

Input Example:
{example_json}

Evaluation Criteria:
1. **Schema Compliance**: Does the assistant response contain <think>, <proof>, and <answer> tags?
2. **Reasoning Quality**: Is the <think> trace logical and step-by-step?
3. **Grounding**: Does the <proof> section contain actual quotes from the <context>?
4. **Correctness**: Is the final <answer> derived strictly from the context?

Output Format:
Return a JSON object:
{
  "score": (0-10),
  "schema_valid": boolean,
  "reasoning_quality": (0-5),
  "grounding_quality": (0-5),
  "critique": "Short explanation..."
}
"""

async def evaluate_example(example: Dict, model: str) -> Dict:
    """Evaluates a single example using the Judge LLM."""
    
    # Extract the assistant content to check tags
    messages = example.get("messages", [])
    if len(messages) < 2:
        return {"score": 0, "critique": "Invalid message format", "schema_valid": False}
    
    assistant_content = messages[1]["content"]
    
    # Quick Regex/String check for tags (heuristic)
    has_think = "<think>" in assistant_content and "</think>" in assistant_content
    has_proof = "<proof>" in assistant_content and "</proof>" in assistant_content
    has_answer = "<answer>" in assistant_content and "</answer>" in assistant_content
    schema_valid_heuristic = has_think and has_proof and has_answer

    try:
        response = await litellm.acompletion(
            model=JUDGE_MODEL,
            messages=[
                {"role": "system", "content": "You are a strict data quality judge. Return ONLY valid JSON."},
                {"role": "user", "content": JUDGE_PROMPT.format(example_json=json.dumps(example))}
            ],
            # response_format={"type": "json_object"}, # Removing this as it might be causing issues with Ollama
            temperature=0.0
        )
        # Debug: Print full response object
        # print(f"DEBUG Full Response: {response}")
        
        content = response.choices[0].message.content
        # Debug print
        print(f"DEBUG Raw Content: {content!r}")
        
        # Clean markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
            
        result = json.loads(content)
        result["model"] = model
        result["heuristic_schema"] = schema_valid_heuristic
        return result
    except Exception as e:
        print(f"Error evaluating: {type(e).__name__}: {e}")
        # Debug: Print raw content if available
        try:
            print(f"Raw content: {response.choices[0].message.content}")
        except:
            pass
        return {
            "score": 0, 
            "schema_valid": False, 
            "critique": f"Evaluation failed: {e}", 
            "model": model,
            "heuristic_schema": schema_valid_heuristic
        }

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--files", nargs="+", required=True, help="List of .jsonl files to compare")
    parser.add_argument("--labels", nargs="+", required=True, help="Labels for the files (e.g. GPT-OSS Qwen)")
    args = parser.parse_args()

    if len(args.files) != len(args.labels):
        print("Error: Number of files must match number of labels.")
        return

    all_results = {}

    for file_path, label in zip(args.files, args.labels):
        print(f"\nEvaluating {label} ({file_path})...")
        examples = []
        with open(file_path, "r") as f:
            for line in f:
                examples.append(json.loads(line))
        
        tasks = [evaluate_example(ex, label) for ex in examples]
        results = []
        for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):
            results.append(await future)
        
        all_results[label] = results

    # Print Summary
    print("\n" + "="*40)
    print("COMPARISON REPORT")
    print("="*40)
    
    for label, results in all_results.items():
        avg_score = sum(r["score"] for r in results) / len(results) if results else 0
        avg_reasoning = sum(r.get("reasoning_quality", 0) for r in results) / len(results) if results else 0
        schema_pass = sum(1 for r in results if r.get("schema_valid"))
        heuristic_pass = sum(1 for r in results if r.get("heuristic_schema"))
        
        print(f"\nModel: {label}")
        print(f"  Examples: {len(results)}")
        print(f"  Avg Score: {avg_score:.2f} / 10")
        print(f"  Avg Reasoning: {avg_reasoning:.2f} / 5")
        print(f"  Schema Compliance (Judge): {schema_pass}/{len(results)}")
        print(f"  Schema Compliance (Strict): {heuristic_pass}/{len(results)}")

if __name__ == "__main__":
    asyncio.run(main())
